{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (CNN) Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Class for logging trainings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataLogger():\n",
    "    '''Helper class that stores informatoion during training'''\n",
    "    def __init__(self) -> None:\n",
    "        self.ls_losses:List[float] = []\n",
    "\n",
    "        '''important for visualization'''\n",
    "        self.ls_epochs:List[int]    = []\n",
    "        self.ls_imgs:List[torch.Tensor]  = []\n",
    "        self.ls_recimgs:List[torch.Tensor]  = [] \n",
    "        pass "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss per epoch\n",
    "def plot_losses(losses) -> None:\n",
    "    '''plots loss per epoch'''\n",
    "    epochs = range(1, len(losses)+1)\n",
    "\n",
    "    xticks = range(min(epochs), max(epochs)+1) # transforms into integer\n",
    "\n",
    "    plt.plot(epochs, losses)\n",
    "    plt.title('Loss per epoch'), plt.ylabel('Loss'),  plt.xlabel('Epoch')\n",
    "    plt.xticks(xticks)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA LOADER\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "\n",
    "data_train = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "data_test  = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(data_train, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(data_test, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\heuer\\GitHub\\tutorial_Python\\libs\\pytorch\\02_project_mnist\\torch02-03_autoencoder_CNN.ipynb Cell 10\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/heuer/GitHub/tutorial_Python/libs/pytorch/02_project_mnist/torch02-03_autoencoder_CNN.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# DATA: look at pixels value range (important for activation function in decoder)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/heuer/GitHub/tutorial_Python/libs/pytorch/02_project_mnist/torch02-03_autoencoder_CNN.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m imgs, labels \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(train_dataloader))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/heuer/GitHub/tutorial_Python/libs/pytorch/02_project_mnist/torch02-03_autoencoder_CNN.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m( torch\u001b[39m.\u001b[39mmin(imgs)  , torch\u001b[39m.\u001b[39mmax(imgs) )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "# DATA: look at pixels value range (important for activation function in decoder)\n",
    "imgs, labels = next(iter(train_dataloader))\n",
    "\n",
    "print( torch.min(imgs)  , torch.max(imgs) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class: Autoencoder (Conv.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "        Normal:\n",
      "            [64, 1, 28, 28]\n",
      "        Convolutional:\n",
      "            [64, 16, 14, 14], \n",
      "            [64, 32, 7, 7], \n",
      "            [64, 64, 1, 1],\n",
      "        Tranpose:    \n",
      "            [64, 32, 7, 7]\n",
      "            [64, 16, 14, 14], \n",
      "            [64, 1, 28, 28],    \n",
      "            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# determine output size\n",
    "conv1 = nn.Conv2d(1, 16, 3, stride=2, padding=1)\n",
    "conv2 = nn.Conv2d(16, 32, 3, stride=2, padding=1)\n",
    "conv3 = nn.Conv2d(32, 64, 7)\n",
    "\n",
    "tconv1 = nn.ConvTranspose2d(64, 32, 7)\n",
    "tconv2 = nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1)\n",
    "tconv3 = nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "\n",
    "imgs, _ = next(iter(train_dataloader))\n",
    "\n",
    "# PRINTS\n",
    "print(f'''\n",
    "\n",
    "        Normal:\n",
    "            {list(imgs.size())}\n",
    "        Convolutional:\n",
    "            {list(conv1(imgs).size())}, \n",
    "            {list(conv2(conv1(imgs)).size())}, \n",
    "            {list(conv3(conv2(conv1(imgs))).size())},\n",
    "        Tranpose:    \n",
    "            {list(tconv1(conv3(conv2(conv1(imgs)))).size())}\n",
    "            {list(tconv2(tconv1(conv3(conv2(conv1(imgs))))).size())}, \n",
    "            {list(tconv3(tconv2(tconv1(conv3(conv2(conv1(imgs)))))).size())},    \n",
    "            \n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder_Conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            conv1, nn.ReLU(),\n",
    "            conv2, nn.ReLU(),\n",
    "            conv3, \n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            tconv1, nn.ReLU(),\n",
    "            tconv2, nn.ReLU(),\n",
    "            tconv3, \n",
    "            nn.Sigmoid() # making pixel in range [0,1] (gray scaled)\n",
    "        )\n",
    "\n",
    "        pass \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def train(epochs: int, model: nn.Module, dataloader: DataLoader, optimizer: optim.Optimizer, criterion) -> TrainDataLogger:\n",
    "    n_batches = len(dataloader)\n",
    "    train_data: TrainDataLogger = TrainDataLogger()\n",
    "    \n",
    "    model.train()\n",
    "    for i_epoch in range(epochs):\n",
    "        loss_epoch = 0\n",
    "        for i_batch, (X,_) in enumerate(dataloader):\n",
    "\n",
    "            # adjust shahpe of tensor, so that it fits into first layer\n",
    "            # X = X.reshape(-1, 28*28) \n",
    "\n",
    "            # pred (encode & decode)\n",
    "            recon = model(X)\n",
    "\n",
    "            # loss (how much difference between each pixel)\n",
    "            loss = criterion(recon, X)\n",
    "            if math.isnan(loss): \n",
    "                print(f'NAN @ epoch={i_epoch}, batch={i_batch}')\n",
    "                return\n",
    "            loss_epoch += loss\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # print\n",
    "            if i_batch % 250 == 0:\n",
    "                print(f'Epoch\\t{i_epoch+1}/{epochs}\\t\\tBatch\\t{i_batch}/{n_batches}\\t({(100. * i_batch / n_batches):.1f}%)\\t\\tLoss\\t{loss:.4f}')\n",
    "        \n",
    "        # Save training data per epoch\n",
    "        train_data.ls_losses.append(loss_epoch / n_batches)\n",
    "        train_data.ls_imgs.append(X)\n",
    "        train_data.ls_recimgs.append(recon)\n",
    "        train_data.ls_epochs.append(i_epoch)\n",
    "\n",
    "\n",
    "    return train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model:nn.Module, dataloader:DataLoader, criterion) -> float:\n",
    "    model.eval()\n",
    "\n",
    "    n_batches = len(dataloader)\n",
    "    loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, (X,_) in enumerate(dataloader):\n",
    "           # X = X.reshape(-1,  28*28)\n",
    "\n",
    "            # pred \n",
    "            recon = model(X)\n",
    "\n",
    "            # loss\n",
    "            loss += criterion(recon, X)\n",
    "\n",
    "        loss = loss / n_batches\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder_Conv() # Autoencoder_Linear()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch\t1/10\t\tBatch\t0/938\t(0.0%)\t\tLoss\t0.2371\n",
      "NAN @ epoch=0, batch=93\n"
     ]
    }
   ],
   "source": [
    "train_data:TrainDataLogger = train(3, model, train_dataloader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'ls_losses'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\heuer\\GitHub\\tutorial_Python\\libs\\pytorch\\02_project_mnist\\torch02-03_autoencoder.ipynb Cell 24\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/heuer/GitHub/tutorial_Python/libs/pytorch/02_project_mnist/torch02-03_autoencoder.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m plot_losses(torch\u001b[39m.\u001b[39mTensor(train_data\u001b[39m.\u001b[39;49mls_losses))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'ls_losses'"
     ]
    }
   ],
   "source": [
    "plot_losses(torch.Tensor(train_data.ls_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, test_dataloader,  criterion).item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = len(train_data.ls_epochs)\n",
    "\n",
    "num_imgs_per_row = 9 \n",
    "num_rows = 2\n",
    "\n",
    "\n",
    "for i_epoch in range(0, num_epochs):\n",
    "    plt.figure(figsize=(9,2))\n",
    "    plt.gray()\n",
    "    imgs    =  train_data.ls_imgs[i_epoch].detach().numpy()\n",
    "    recimgs  = train_data.ls_recimgs[i_epoch].detach().numpy()\n",
    "\n",
    "    for i, imgs_batch in enumerate(imgs):\n",
    "        if i >= num_imgs_per_row : break\n",
    "\n",
    "        plt.subplot(num_rows,num_imgs_per_row, i+1)\n",
    "        imgs_batch = imgs_batch.reshape(-1, 28, 28)\n",
    "        plt.imshow(imgs_batch[0])\n",
    "\n",
    "    for i, imgs_batch in enumerate(recimgs):\n",
    "        if i >= num_imgs_per_row : break\n",
    "\n",
    "        plt.subplot(num_rows,num_imgs_per_row, i+1+num_imgs_per_row)\n",
    "        imgs_batch = imgs_batch.reshape(-1, 28, 28)\n",
    "        plt.imshow(imgs_batch[0])\n",
    "\n",
    "plt.show()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
