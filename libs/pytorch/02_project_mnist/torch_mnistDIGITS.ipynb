{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn # Layers\n",
    "import torch.nn.functional as F # activation functions\n",
    "import torch.optim as optim # optimizer (SGD)\n",
    "\n",
    "from torch.utils.data import DataLoader # Dataloader (for automatic batches)\n",
    "from torchvision import datasets, transforms # datasets (MNIST) & transformations for pre-processing data (convert to Tensor, Normalization)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10 # Digits [0,9]\n",
    "\n",
    "# Hyperparameter\n",
    "num_epochs = 3\n",
    "batch_size = 64 * 2\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_transformations = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(0.1307, 0.3081)\n",
    "    ])\n",
    "\n",
    "train_data = datasets.MNIST(train=True, root='data-mnist',  download=True, transform=preprocessing_transformations)\n",
    "test_data = datasets.MNIST(train=False, root='data-mnist', download=True, transform=preprocessing_transformations)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is torch.Size([128, 64, 5, 5]), \n",
      "so Fully-Connected Layer input layer has size 64*5*5=1600\n"
     ]
    }
   ],
   "source": [
    "# get random images\n",
    "dataiter = iter(train_dataloader)\n",
    "images, _ = next(dataiter)\n",
    "\n",
    "# COMPUTE INPUT SIZE of first fully-connected layer (therefore define all layers)\n",
    "conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3) # because MNIST is black/white there is only 1 input_channel (RGB images have 3)\n",
    "pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "conv2 = nn.Conv2d(32, 64, 3)\n",
    "pool2 = nn.MaxPool2d(2,2)\n",
    "\n",
    "img_shape = pool2(conv2(pool1(conv1(images)))).size()\n",
    "fc1_input_size = img_shape[1]*img_shape[2]*img_shape[3]\n",
    "print(f'''Shape is {img_shape}, \n",
    "so Fully-Connected Layer input layer has size {img_shape[1]}*{img_shape[2]}*{img_shape[3]}={img_shape[1]*img_shape[2]*img_shape[3]}''')\n",
    "\n",
    "\n",
    "# Define CNN Architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''Define Layers'''\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = conv1\n",
    "        self.pool1 = pool1\n",
    "\n",
    "        self.conv2 = conv2 \n",
    "        self.pool2 = pool2 \n",
    "        \n",
    "        self.dropout = nn.Dropout2d(p=0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=fc1_input_size, out_features=128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Define order of computation & activation functions'''\n",
    "\n",
    "        #  conv. & pool. layers\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # flatten data for fully-connected layers\n",
    "        x = torch.flatten(x,1)\n",
    "\n",
    "        # fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        output = self.fc2(x)\n",
    "        return output\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions - Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs:int, model:nn.Module, optimizer:optim.Optimizer, dataloader:DataLoader, loss_fn):\n",
    "    # tell model to activate training mode (different behaviour of Dropout-Layer compared to when testing)\n",
    "    model.train()\n",
    "    n = len(dataloader)\n",
    "\n",
    "    for i_epoch in range(epochs):\n",
    "        for i_batch, (X,y) in enumerate(dataloader):\n",
    "            # prediction\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y) # pred is 1. arguement, y is 2. !!!\n",
    "\n",
    "            # backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print\n",
    "            if i_batch % 10 == 0:\n",
    "                print(f'Epoch\\t{i_epoch+1}/{num_epochs}\\t\\tBatch {i_batch}\\{n}\\t({(100. * i_batch / n):.1f}%)\\t\\tLoss {loss:.4f}')\n",
    "\n",
    "\n",
    "def test(model:nn.Module, dataloader:DataLoader):\n",
    "    model.eval()\n",
    "\n",
    "    num_samples = 0\n",
    "    num_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, (X,y) in enumerate(dataloader):\n",
    "            # predict & calculate loss\n",
    "            pred = model(X)\n",
    "            _, pred = torch.max(pred, 1)\n",
    "            \n",
    "            # count\n",
    "            num_samples += y.size(0)\n",
    "            num_correct += (pred==y).sum().item()\n",
    "\n",
    "        acc = 100 * num_correct / num_samples\n",
    "        print(f'Accuracy is {acc:.2f}%') \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main - Train & Test/Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "optim = optim.SGD(model.parameters(),lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch\t1/3\t\tBatch 0\\469\t(0.0%)\t\tLoss 2.2915\n",
      "Epoch\t1/3\t\tBatch 10\\469\t(2.1%)\t\tLoss 1.8266\n",
      "Epoch\t1/3\t\tBatch 20\\469\t(4.3%)\t\tLoss 1.5113\n",
      "Epoch\t1/3\t\tBatch 30\\469\t(6.4%)\t\tLoss 0.8972\n",
      "Epoch\t1/3\t\tBatch 40\\469\t(8.5%)\t\tLoss 0.4946\n",
      "Epoch\t1/3\t\tBatch 50\\469\t(10.7%)\t\tLoss 0.5154\n",
      "Epoch\t1/3\t\tBatch 60\\469\t(12.8%)\t\tLoss 0.3556\n",
      "Epoch\t1/3\t\tBatch 70\\469\t(14.9%)\t\tLoss 0.3764\n",
      "Epoch\t1/3\t\tBatch 80\\469\t(17.1%)\t\tLoss 0.2105\n",
      "Epoch\t1/3\t\tBatch 90\\469\t(19.2%)\t\tLoss 0.3042\n",
      "Epoch\t1/3\t\tBatch 100\\469\t(21.3%)\t\tLoss 0.3798\n",
      "Epoch\t1/3\t\tBatch 110\\469\t(23.5%)\t\tLoss 0.2470\n",
      "Epoch\t1/3\t\tBatch 120\\469\t(25.6%)\t\tLoss 0.2667\n",
      "Epoch\t1/3\t\tBatch 130\\469\t(27.7%)\t\tLoss 0.1850\n",
      "Epoch\t1/3\t\tBatch 140\\469\t(29.9%)\t\tLoss 0.2269\n",
      "Epoch\t1/3\t\tBatch 150\\469\t(32.0%)\t\tLoss 0.2484\n",
      "Epoch\t1/3\t\tBatch 160\\469\t(34.1%)\t\tLoss 0.2348\n",
      "Epoch\t1/3\t\tBatch 170\\469\t(36.2%)\t\tLoss 0.1245\n",
      "Epoch\t1/3\t\tBatch 180\\469\t(38.4%)\t\tLoss 0.2526\n",
      "Epoch\t1/3\t\tBatch 190\\469\t(40.5%)\t\tLoss 0.1207\n",
      "Epoch\t1/3\t\tBatch 200\\469\t(42.6%)\t\tLoss 0.1685\n",
      "Epoch\t1/3\t\tBatch 210\\469\t(44.8%)\t\tLoss 0.1921\n",
      "Epoch\t1/3\t\tBatch 220\\469\t(46.9%)\t\tLoss 0.1577\n",
      "Epoch\t1/3\t\tBatch 230\\469\t(49.0%)\t\tLoss 0.1640\n",
      "Epoch\t1/3\t\tBatch 240\\469\t(51.2%)\t\tLoss 0.1663\n",
      "Epoch\t1/3\t\tBatch 250\\469\t(53.3%)\t\tLoss 0.1138\n",
      "Epoch\t1/3\t\tBatch 260\\469\t(55.4%)\t\tLoss 0.2023\n",
      "Epoch\t1/3\t\tBatch 270\\469\t(57.6%)\t\tLoss 0.1395\n",
      "Epoch\t1/3\t\tBatch 280\\469\t(59.7%)\t\tLoss 0.1221\n",
      "Epoch\t1/3\t\tBatch 290\\469\t(61.8%)\t\tLoss 0.0977\n",
      "Epoch\t1/3\t\tBatch 300\\469\t(64.0%)\t\tLoss 0.3007\n",
      "Epoch\t1/3\t\tBatch 310\\469\t(66.1%)\t\tLoss 0.0956\n",
      "Epoch\t1/3\t\tBatch 320\\469\t(68.2%)\t\tLoss 0.1603\n",
      "Epoch\t1/3\t\tBatch 330\\469\t(70.4%)\t\tLoss 0.1866\n",
      "Epoch\t1/3\t\tBatch 340\\469\t(72.5%)\t\tLoss 0.0620\n",
      "Epoch\t1/3\t\tBatch 350\\469\t(74.6%)\t\tLoss 0.2086\n",
      "Epoch\t1/3\t\tBatch 360\\469\t(76.8%)\t\tLoss 0.1328\n",
      "Epoch\t1/3\t\tBatch 370\\469\t(78.9%)\t\tLoss 0.2184\n",
      "Epoch\t1/3\t\tBatch 380\\469\t(81.0%)\t\tLoss 0.1991\n",
      "Epoch\t1/3\t\tBatch 390\\469\t(83.2%)\t\tLoss 0.0688\n",
      "Epoch\t1/3\t\tBatch 400\\469\t(85.3%)\t\tLoss 0.1143\n",
      "Epoch\t1/3\t\tBatch 410\\469\t(87.4%)\t\tLoss 0.1145\n",
      "Epoch\t1/3\t\tBatch 420\\469\t(89.6%)\t\tLoss 0.1001\n",
      "Epoch\t1/3\t\tBatch 430\\469\t(91.7%)\t\tLoss 0.0629\n",
      "Epoch\t1/3\t\tBatch 440\\469\t(93.8%)\t\tLoss 0.1300\n",
      "Epoch\t1/3\t\tBatch 450\\469\t(95.9%)\t\tLoss 0.1206\n",
      "Epoch\t1/3\t\tBatch 460\\469\t(98.1%)\t\tLoss 0.1130\n",
      "Epoch\t2/3\t\tBatch 0\\469\t(0.0%)\t\tLoss 0.1871\n",
      "Epoch\t2/3\t\tBatch 10\\469\t(2.1%)\t\tLoss 0.1672\n",
      "Epoch\t2/3\t\tBatch 20\\469\t(4.3%)\t\tLoss 0.1423\n",
      "Epoch\t2/3\t\tBatch 30\\469\t(6.4%)\t\tLoss 0.1054\n",
      "Epoch\t2/3\t\tBatch 40\\469\t(8.5%)\t\tLoss 0.0897\n",
      "Epoch\t2/3\t\tBatch 50\\469\t(10.7%)\t\tLoss 0.0490\n",
      "Epoch\t2/3\t\tBatch 60\\469\t(12.8%)\t\tLoss 0.0776\n",
      "Epoch\t2/3\t\tBatch 70\\469\t(14.9%)\t\tLoss 0.1008\n",
      "Epoch\t2/3\t\tBatch 80\\469\t(17.1%)\t\tLoss 0.0869\n",
      "Epoch\t2/3\t\tBatch 90\\469\t(19.2%)\t\tLoss 0.1091\n",
      "Epoch\t2/3\t\tBatch 100\\469\t(21.3%)\t\tLoss 0.0555\n",
      "Epoch\t2/3\t\tBatch 110\\469\t(23.5%)\t\tLoss 0.0838\n",
      "Epoch\t2/3\t\tBatch 120\\469\t(25.6%)\t\tLoss 0.0607\n",
      "Epoch\t2/3\t\tBatch 130\\469\t(27.7%)\t\tLoss 0.0645\n",
      "Epoch\t2/3\t\tBatch 140\\469\t(29.9%)\t\tLoss 0.0654\n",
      "Epoch\t2/3\t\tBatch 150\\469\t(32.0%)\t\tLoss 0.1646\n",
      "Epoch\t2/3\t\tBatch 160\\469\t(34.1%)\t\tLoss 0.1079\n",
      "Epoch\t2/3\t\tBatch 170\\469\t(36.2%)\t\tLoss 0.1475\n",
      "Epoch\t2/3\t\tBatch 180\\469\t(38.4%)\t\tLoss 0.0598\n",
      "Epoch\t2/3\t\tBatch 190\\469\t(40.5%)\t\tLoss 0.1011\n",
      "Epoch\t2/3\t\tBatch 200\\469\t(42.6%)\t\tLoss 0.1536\n",
      "Epoch\t2/3\t\tBatch 210\\469\t(44.8%)\t\tLoss 0.0368\n",
      "Epoch\t2/3\t\tBatch 220\\469\t(46.9%)\t\tLoss 0.1181\n",
      "Epoch\t2/3\t\tBatch 230\\469\t(49.0%)\t\tLoss 0.0904\n",
      "Epoch\t2/3\t\tBatch 240\\469\t(51.2%)\t\tLoss 0.0862\n",
      "Epoch\t2/3\t\tBatch 250\\469\t(53.3%)\t\tLoss 0.0360\n",
      "Epoch\t2/3\t\tBatch 260\\469\t(55.4%)\t\tLoss 0.0651\n",
      "Epoch\t2/3\t\tBatch 270\\469\t(57.6%)\t\tLoss 0.1210\n",
      "Epoch\t2/3\t\tBatch 280\\469\t(59.7%)\t\tLoss 0.0894\n",
      "Epoch\t2/3\t\tBatch 290\\469\t(61.8%)\t\tLoss 0.0717\n",
      "Epoch\t2/3\t\tBatch 300\\469\t(64.0%)\t\tLoss 0.0907\n",
      "Epoch\t2/3\t\tBatch 310\\469\t(66.1%)\t\tLoss 0.0601\n",
      "Epoch\t2/3\t\tBatch 320\\469\t(68.2%)\t\tLoss 0.0806\n",
      "Epoch\t2/3\t\tBatch 330\\469\t(70.4%)\t\tLoss 0.0762\n",
      "Epoch\t2/3\t\tBatch 340\\469\t(72.5%)\t\tLoss 0.0948\n",
      "Epoch\t2/3\t\tBatch 350\\469\t(74.6%)\t\tLoss 0.1361\n",
      "Epoch\t2/3\t\tBatch 360\\469\t(76.8%)\t\tLoss 0.0995\n",
      "Epoch\t2/3\t\tBatch 370\\469\t(78.9%)\t\tLoss 0.0661\n",
      "Epoch\t2/3\t\tBatch 380\\469\t(81.0%)\t\tLoss 0.1086\n",
      "Epoch\t2/3\t\tBatch 390\\469\t(83.2%)\t\tLoss 0.1341\n",
      "Epoch\t2/3\t\tBatch 400\\469\t(85.3%)\t\tLoss 0.0801\n",
      "Epoch\t2/3\t\tBatch 410\\469\t(87.4%)\t\tLoss 0.0679\n",
      "Epoch\t2/3\t\tBatch 420\\469\t(89.6%)\t\tLoss 0.0660\n",
      "Epoch\t2/3\t\tBatch 430\\469\t(91.7%)\t\tLoss 0.1524\n",
      "Epoch\t2/3\t\tBatch 440\\469\t(93.8%)\t\tLoss 0.1393\n",
      "Epoch\t2/3\t\tBatch 450\\469\t(95.9%)\t\tLoss 0.0826\n",
      "Epoch\t2/3\t\tBatch 460\\469\t(98.1%)\t\tLoss 0.0635\n",
      "Epoch\t3/3\t\tBatch 0\\469\t(0.0%)\t\tLoss 0.0250\n",
      "Epoch\t3/3\t\tBatch 10\\469\t(2.1%)\t\tLoss 0.0613\n",
      "Epoch\t3/3\t\tBatch 20\\469\t(4.3%)\t\tLoss 0.0669\n",
      "Epoch\t3/3\t\tBatch 30\\469\t(6.4%)\t\tLoss 0.0392\n",
      "Epoch\t3/3\t\tBatch 40\\469\t(8.5%)\t\tLoss 0.1139\n",
      "Epoch\t3/3\t\tBatch 50\\469\t(10.7%)\t\tLoss 0.0550\n",
      "Epoch\t3/3\t\tBatch 60\\469\t(12.8%)\t\tLoss 0.0549\n",
      "Epoch\t3/3\t\tBatch 70\\469\t(14.9%)\t\tLoss 0.0872\n",
      "Epoch\t3/3\t\tBatch 80\\469\t(17.1%)\t\tLoss 0.1130\n",
      "Epoch\t3/3\t\tBatch 90\\469\t(19.2%)\t\tLoss 0.0398\n",
      "Epoch\t3/3\t\tBatch 100\\469\t(21.3%)\t\tLoss 0.0757\n",
      "Epoch\t3/3\t\tBatch 110\\469\t(23.5%)\t\tLoss 0.0550\n",
      "Epoch\t3/3\t\tBatch 120\\469\t(25.6%)\t\tLoss 0.0170\n",
      "Epoch\t3/3\t\tBatch 130\\469\t(27.7%)\t\tLoss 0.0808\n",
      "Epoch\t3/3\t\tBatch 140\\469\t(29.9%)\t\tLoss 0.0623\n",
      "Epoch\t3/3\t\tBatch 150\\469\t(32.0%)\t\tLoss 0.1173\n",
      "Epoch\t3/3\t\tBatch 160\\469\t(34.1%)\t\tLoss 0.0721\n",
      "Epoch\t3/3\t\tBatch 170\\469\t(36.2%)\t\tLoss 0.0606\n",
      "Epoch\t3/3\t\tBatch 180\\469\t(38.4%)\t\tLoss 0.1490\n",
      "Epoch\t3/3\t\tBatch 190\\469\t(40.5%)\t\tLoss 0.0554\n",
      "Epoch\t3/3\t\tBatch 200\\469\t(42.6%)\t\tLoss 0.0474\n",
      "Epoch\t3/3\t\tBatch 210\\469\t(44.8%)\t\tLoss 0.0586\n",
      "Epoch\t3/3\t\tBatch 220\\469\t(46.9%)\t\tLoss 0.0839\n",
      "Epoch\t3/3\t\tBatch 230\\469\t(49.0%)\t\tLoss 0.0351\n",
      "Epoch\t3/3\t\tBatch 240\\469\t(51.2%)\t\tLoss 0.0583\n",
      "Epoch\t3/3\t\tBatch 250\\469\t(53.3%)\t\tLoss 0.1146\n",
      "Epoch\t3/3\t\tBatch 260\\469\t(55.4%)\t\tLoss 0.0315\n",
      "Epoch\t3/3\t\tBatch 270\\469\t(57.6%)\t\tLoss 0.0679\n",
      "Epoch\t3/3\t\tBatch 280\\469\t(59.7%)\t\tLoss 0.0884\n",
      "Epoch\t3/3\t\tBatch 290\\469\t(61.8%)\t\tLoss 0.0515\n",
      "Epoch\t3/3\t\tBatch 300\\469\t(64.0%)\t\tLoss 0.0994\n",
      "Epoch\t3/3\t\tBatch 310\\469\t(66.1%)\t\tLoss 0.0745\n",
      "Epoch\t3/3\t\tBatch 320\\469\t(68.2%)\t\tLoss 0.0210\n",
      "Epoch\t3/3\t\tBatch 330\\469\t(70.4%)\t\tLoss 0.1461\n",
      "Epoch\t3/3\t\tBatch 340\\469\t(72.5%)\t\tLoss 0.0293\n",
      "Epoch\t3/3\t\tBatch 350\\469\t(74.6%)\t\tLoss 0.0510\n",
      "Epoch\t3/3\t\tBatch 360\\469\t(76.8%)\t\tLoss 0.0496\n",
      "Epoch\t3/3\t\tBatch 370\\469\t(78.9%)\t\tLoss 0.0721\n",
      "Epoch\t3/3\t\tBatch 380\\469\t(81.0%)\t\tLoss 0.0888\n",
      "Epoch\t3/3\t\tBatch 390\\469\t(83.2%)\t\tLoss 0.0774\n",
      "Epoch\t3/3\t\tBatch 400\\469\t(85.3%)\t\tLoss 0.0492\n",
      "Epoch\t3/3\t\tBatch 410\\469\t(87.4%)\t\tLoss 0.0459\n",
      "Epoch\t3/3\t\tBatch 420\\469\t(89.6%)\t\tLoss 0.0747\n",
      "Epoch\t3/3\t\tBatch 430\\469\t(91.7%)\t\tLoss 0.0770\n",
      "Epoch\t3/3\t\tBatch 440\\469\t(93.8%)\t\tLoss 0.0775\n",
      "Epoch\t3/3\t\tBatch 450\\469\t(95.9%)\t\tLoss 0.1125\n",
      "Epoch\t3/3\t\tBatch 460\\469\t(98.1%)\t\tLoss 0.0836\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    epochs=num_epochs,\n",
    "    model=model,\n",
    "    optimizer=optim, \n",
    "    loss_fn=criterion, \n",
    "    dataloader=train_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 98.84%\n"
     ]
    }
   ],
   "source": [
    "test(model=model, dataloader=test_dataloader)\n",
    "# 99.78"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
