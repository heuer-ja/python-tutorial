{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Own Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Create NN class\n",
    "- `nn.Linear(a, b)` - creates Linear Layer with a inputs and b outputs\n",
    "\n",
    "Feed NN & Output values\n",
    "- `input = Variable(torch.Tensor(...))`\n",
    "- `my_net(input)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn # super constructor, layers\n",
    "import torch.nn.functional as F # activation functions\n",
    "from torch.autograd import Variable # used for inputs\n",
    "import torch.optim as optim # handles training nn\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create (Own) Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNetwork(\n",
      "  (layer1): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (layer2): Linear(in_features=5, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# class inherits nn.Module\n",
    "class MyNetwork(nn.Module): \n",
    "    def __init__(self) -> None:\n",
    "        ''' define basic architecture (#layers) of NN '''\n",
    "        # setup network \n",
    "        super(MyNetwork, self).__init__()\n",
    "\n",
    "        # DEFINE LAYERS (Architecture)\n",
    "        l1_in = 10\n",
    "        l1_out = 5\n",
    "        l2_in = l1_out\n",
    "        l2_out = 10\n",
    "\n",
    "        self.layer1 = nn.Linear(l1_in, l1_out) \n",
    "        self.layer2 = nn.Linear (l2_in, l2_out)\n",
    "        \n",
    "\n",
    "    def  forward(self, x):\n",
    "        ''' define act. functions for hidden layers         \n",
    "        Note:last layer is output, so no act. function\n",
    "        ''' \n",
    "        x = F.relu(self.layer1(x))\n",
    "        pred = self.layer2(x)\n",
    "        return pred\n",
    "\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        '''dont know'''\n",
    "        size =  x.size()[1:] # only look at one sample\n",
    "        num =  1\n",
    "        for i in size:\n",
    "            num *= i\n",
    "        return num\n",
    "\n",
    "\n",
    "my_network1 = MyNetwork()\n",
    "print(my_network1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2137, -0.2316,  0.2761, -0.6808, -1.0573,  0.6701, -0.2071,  0.3985,\n",
       "          0.3899, -0.1063],\n",
       "        [ 0.5424, -0.1629,  0.1555, -0.1053, -0.3738,  0.4691, -0.2534,  0.2562,\n",
       "         -0.4020, -0.3528],\n",
       "        [ 0.5053, -0.1936,  0.0598, -0.2872, -0.3856,  0.4570, -0.2593,  0.1797,\n",
       "         -0.3505, -0.4848],\n",
       "        [ 0.3968, -0.0971,  0.2047, -0.2314, -0.5146,  0.5892, -0.2251,  0.3069,\n",
       "         -0.2766, -0.3448],\n",
       "        [ 0.4402, -0.0298,  0.2956, -0.2012, -0.5356,  0.6560, -0.2445,  0.3906,\n",
       "         -0.2767, -0.3256],\n",
       "        [ 0.0983, -0.4261, -0.0438, -0.3734, -0.7264,  0.4279, -0.4094,  0.0741,\n",
       "          0.1209, -0.0582],\n",
       "        [ 0.3400, -0.2554, -0.0420, -0.1773, -0.6116,  0.6007, -0.5579,  0.0539,\n",
       "         -0.0646, -0.0500],\n",
       "        [ 0.0731, -0.1241,  0.2561, -0.5032, -0.7524,  0.6273, -0.1018,  0.3737,\n",
       "         -0.0134, -0.3405],\n",
       "        [-0.1284,  0.0086,  0.7308, -0.6593, -1.1734,  0.8583, -0.1261,  0.8355,\n",
       "          0.4219, -0.1071],\n",
       "        [ 0.6674,  0.0089,  0.3449, -0.0053, -0.4233,  0.6612, -0.3559,  0.4249,\n",
       "         -0.4019, -0.2751]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variables is a Wrapper for Tensor so that a Tensor can be used for processing in Neurat Networks, e. g., in backward-prop.\n",
    "\n",
    "l1_input_size = 10\n",
    "amount_data_samples = 10\n",
    "input = Variable(torch.randn(l1_input_size, amount_data_samples))\n",
    "\n",
    "\n",
    "# output some values (without training)\n",
    "output = my_network1(input)\n",
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following NN learns how flip bits of an fixed binary array of length 10.\n",
    "\n",
    "The array is `[1,0,1,1,1,1,1,0,0,0]` and therefore the network should predict `[0,1,0,0,0,0,0,1,1,1]`.\n",
    "\n",
    "X: `[1,0,1,1,1,1,1,0,0,0]`\n",
    "\n",
    "y: `[0,1,0,0,0,0,0,1,1,1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X,y, model, loss_fn, optimizer):\n",
    "    # Compute prediction and loss\n",
    "    pred = model(X)\n",
    "    loss = loss_fn(pred,y)\n",
    "    print(loss)\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()   # reset gradients to prevent summing up errors\n",
    "    loss.backward()         # calculate\n",
    "    optimizer.step()        # update weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4157, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3808, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3502, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3240, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3008, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2786, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2572, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2367, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2171, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1983, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1804, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1634, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1474, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1325, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1186, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1058, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0940, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0833, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0736, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0648, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0570, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0499, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0437, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0382, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0333, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0291, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0253, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0220, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0191, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0166, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0144, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0125, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0109, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0094, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0082, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0071, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0061, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0046, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# setup network\n",
    "my_network2 = MyNetwork()\n",
    "\n",
    "# choose hyperparameter\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(my_network2.parameters(), lr=0.1)\n",
    "\n",
    "# create training data\n",
    "batch = 7\n",
    "X = Variable(torch.Tensor([[1,0,1,1,1,1,1,0,0,0]for _ in range(batch)]))\n",
    "y = Variable(torch.Tensor([[0,1,0,0,0,0,0,1,1,1]for _ in range(batch)]))\n",
    "\n",
    "# TRAINING\n",
    "epochs = 50\n",
    "for e in range(epochs):\n",
    "    train(X,y, my_network2, loss_fn, optimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same looking arrays create low loss: 0.0008 \n",
      "\n",
      "Different looking arrays create high loss: 0.552\n"
     ]
    }
   ],
   "source": [
    "# Test 1: same data point\n",
    "test1_X = Variable(torch.Tensor([1,0,1,1,1,1,1,0,0,0]))\n",
    "test1_Y = Variable(torch.Tensor([0,1,0,0,0,0,0,1,1,1]))\n",
    "\n",
    "pred1 = my_network2(test1_X)\n",
    "loss1 = loss_fn(pred1,test1_Y).item()\n",
    "print(f\"Same looking arrays create low loss: {round(loss1, 4)} \")\n",
    "\n",
    "\n",
    "test2_X = Variable(torch.Tensor([1,1,1,1,1,1,1,1,1,1]))\n",
    "test2_Y = Variable(torch.Tensor([0,0,0,0,0,0,0,0,0,0]))\n",
    "\n",
    "pred2 = my_network2(test2_X)\n",
    "loss2 = loss_fn(pred2,test2_Y).item()\n",
    "print(f\"\\nDifferent looking arrays create high loss: {round(loss2, 4)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
